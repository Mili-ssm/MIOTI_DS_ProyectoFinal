{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")  # go to parent dir\n",
    "DATA_SRC = os.path.join(\"..\", \"Data\")\n",
    "\n",
    "data_analysis = 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import CustomDataFrame\n",
    "\n",
    "path = os.path.join(DATA_SRC, \"CSV\", \"train.csv\")\n",
    "\n",
    "data_df = CustomDataFrame(\n",
    "    [path],\n",
    "    index_col=\"id\",\n",
    "    target_cols=[\"sii\"],\n",
    "    no_null_cols=[\"sii\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Calcula el porcentaje de valores nulos por columna\n",
    "nulls_info_load_ = data_df.null_percent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Nulls (Columns/Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import Show\n",
    "\n",
    "data_df = data_df.clean_nulls(0.5, show=Show.PLOT)\n",
    "after_auto_clean_ = data_df.dataframe\n",
    "display(after_auto_clean_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Remove Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "drop_columns = [\n",
    "    # LEKEA EL TARGET\n",
    "    \"PCIAT-PCIAT_Total\",\n",
    "    \"PCIAT-PCIAT_01\",\n",
    "    \"PCIAT-PCIAT_02\",\n",
    "    \"PCIAT-PCIAT_03\",\n",
    "    \"PCIAT-PCIAT_04\",\n",
    "    \"PCIAT-PCIAT_05\",\n",
    "    \"PCIAT-PCIAT_06\",\n",
    "    \"PCIAT-PCIAT_07\",\n",
    "    \"PCIAT-PCIAT_08\",\n",
    "    \"PCIAT-PCIAT_09\",\n",
    "    \"PCIAT-PCIAT_10\",\n",
    "    \"PCIAT-PCIAT_11\",\n",
    "    \"PCIAT-PCIAT_12\",\n",
    "    \"PCIAT-PCIAT_13\",\n",
    "    \"PCIAT-PCIAT_14\",\n",
    "    \"PCIAT-PCIAT_15\",\n",
    "    \"PCIAT-PCIAT_16\",\n",
    "    \"PCIAT-PCIAT_17\",\n",
    "    \"PCIAT-PCIAT_18\",\n",
    "    \"PCIAT-PCIAT_19\",\n",
    "    \"PCIAT-PCIAT_20\",\n",
    "]\n",
    "#  display(data.dataframe.columns)\n",
    "\n",
    "print(f\"Columns Count BEFORE DROP: {len(data_df.dataframe.columns)}\")\n",
    "data_df.dataframe = data_df.dataframe.drop([\n",
    "    col for col in drop_columns if col in data_df.dataframe.columns\n",
    "])\n",
    "\n",
    "print(f\"Columns Count AFTER DROP: {len(data_df.dataframe.columns)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Data Shape: {data_df.dataframe.shape}\")\n",
    "print(\"Data Columns:\")\n",
    "pprint(data_df.dataframe.columns)\n",
    "\n",
    "data_df.null_percent()\n",
    "\n",
    "data_null_cols_clean_ = data_df.dataframe\n",
    "display(data_null_cols_clean_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from utils.transform import date_to_cyclic\n",
    "\n",
    "data_df.column_distribution(\"sii\")\n",
    "display(data_df[\"sii\"].unique().sort())\n",
    "\n",
    "ciclyc_date_columns = []\n",
    "for col in ciclyc_date_columns:\n",
    "    if col not in data_df.dataframe.columns:\n",
    "        continue\n",
    "    if data_df.dataframe[col].dtype == pl.String:\n",
    "        data_df.dataframe = data_df.dataframe.with_columns(pl.col(col).str.to_datetime())\n",
    "    data_df.dataframe = date_to_cyclic(data_df.dataframe, col, fill_nan=False)\n",
    "\n",
    "print(\"String Columns:\", data_df.get_columns_types().get(pl.String))\n",
    "\n",
    "\n",
    "non_str_categories_columns = []\n",
    "for col in non_str_categories_columns:\n",
    "    data_df.with_columns(pl.col(col).cast(pl.String))\n",
    "\n",
    "data_df.with_columns(cs.string().cast(pl.Categorical))\n",
    "new_types = data_df.get_columns_types()\n",
    "print(\"New Types: \")\n",
    "pprint(new_types)\n",
    "if True:\n",
    "    dummy_vars: list[str] = []\n",
    "    dummy_vars += new_types.get(pl.Categorical(\"physical\"), [])\n",
    "    dummy_vars += new_types.get(pl.Categorical(\"lexical\"), [])\n",
    "    dummy_vars = [col for col in dummy_vars if col not in data_df.target_cols]\n",
    "\n",
    "    data_df.dataframe = data_df.dataframe.to_dummies(dummy_vars, drop_first=True)\n",
    "\n",
    "    after_dummies_ = data_df.dataframe\n",
    "    display(after_dummies_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_wo_nulls_ = data_df.corr(False, dpi=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForestClassifier\n",
    "\n",
    "ai_df = data_df.dataframe.select(pl.exclude(data_df.index_col))\n",
    "# .filter(    pl.col(\"ResultadoDeLaLlamada\").is_in([\"CU+\", \"CU-\", \"CNU\"]))\n",
    "\n",
    "# prueba.to_csv('data_selected_modelo2.csv', index=False)\n",
    "# df_final = pd.read_csv(\"data_selected_modelo2.csv\")\n",
    "target = \"sii\"\n",
    "# Features/target split\n",
    "target_data = ai_df.select(pl.col(target))\n",
    "display(target_data.head())\n",
    "\n",
    "input_data = ai_df.select(cs.exclude(target))\n",
    "display(input_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data to improve the regression's performance\n",
    "\n",
    "# Define the model\n",
    "import time\n",
    "from collections.abc import Callable\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def calc_time[**Args, R](func: Callable[Args, R]) -> Callable[Args, R]:\n",
    "    @wraps(func)\n",
    "    def wrapper(*args: Args.args, **kwargs: Args.kwargs) -> R:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time: {end_time - start_time} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def feature_importances_calc(X, y) -> None:\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "    # Fit the model\n",
    "    calc_time(model.fit)(X, y.to_numpy().ravel())\n",
    "\n",
    "    # Obtener las importancias de las características\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Crear un DataFrame para mostrar la importancia de cada característica\n",
    "    feature_importances = pl.DataFrame({\n",
    "        \"Feature\": X.columns,\n",
    "        \"Importance\": importances,\n",
    "    }).sort(\n",
    "        by=\"Importance\",\n",
    "        descending=True,\n",
    "    )\n",
    "\n",
    "    display(feature_importances)\n",
    "\n",
    "\n",
    "print(\"All Features:\")\n",
    "feature_importances_calc(input_data, target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train test split\n",
    "print(\"Train test split\")\n",
    "data_split: tuple[pl.DataFrame, pl.DataFrame, ndarray, ndarray] = train_test_split(  # type: ignore\n",
    "    input_data.select(sorted(input_data.columns)),\n",
    "    target_data.to_numpy().ravel(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "input_train, input_test, target_train, target_test = data_split\n",
    "print(f\"X_train shape: {input_train.shape}\")\n",
    "print(f\"X_test  shape: {input_test.shape}\")\n",
    "display(\n",
    "    pl.Series(name=\"cats\", values=target_train)\n",
    "    .value_counts()\n",
    "    .sort(by=\"count\")\n",
    "    .plot.bar(x=\"cats\", y=\"count\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions analysis\n",
    "\n",
    "* Confusion matrix and testing metrics\n",
    "* Output probabilities distribution\n",
    "* Feature importances using different criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pretty_confusion_matrix import pp_matrix_from_data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay,\n",
    "    accuracy_score,\n",
    "    fbeta_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "display(target_test)\n",
    "display(type(target_test))\n",
    "\n",
    "\n",
    "def train_rnd_forest(X_train, y_train) -> RandomForestClassifier:\n",
    "    print(\"Start training the model\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=25,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_metrics(\n",
    "    model: RandomForestClassifier,\n",
    "    input_test: pl.DataFrame,\n",
    "    target_test: ndarray,\n",
    ") -> None:\n",
    "    print(\"Model Predictions\")\n",
    "    predictions = model.predict(input_test)\n",
    "\n",
    "    print(\"Accuracy: \", accuracy_score(target_test, predictions))\n",
    "    print(\"Fbeta score: \", fbeta_score(target_test, predictions, beta=0.5, average=\"weighted\"))\n",
    "    print(\"Recall score: \", recall_score(target_test, predictions, average=\"weighted\"))\n",
    "    print(\"Precision score: \", precision_score(target_test, predictions, average=\"weighted\"))\n",
    "\n",
    "    figsize = len(model.classes_) + 2\n",
    "    pp_matrix_from_data(\n",
    "        target_test,\n",
    "        predictions,\n",
    "        columns=model.classes_.tolist(),\n",
    "        cmap=\"PuRd\",\n",
    "        figsize=(figsize, figsize),\n",
    "    )\n",
    "    if len(model.classes_) == 2:\n",
    "        RocCurveDisplay.from_estimator(\n",
    "            model,\n",
    "            input_test,\n",
    "            target_test,\n",
    "            pos_label=model.classes_[0],\n",
    "        )\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "\n",
    "def model_analisys(\n",
    "    input_train: pl.DataFrame,\n",
    "    target_train: ndarray,\n",
    "    input_test: pl.DataFrame,\n",
    "    target_test: ndarray,\n",
    "    labels: list | None = None,\n",
    "    model: RandomForestClassifier | None = None,\n",
    ") -> tuple[RandomForestClassifier, tuple[ndarray, ndarray]]:\n",
    "    data_train = target_train\n",
    "    data_test = target_test\n",
    "\n",
    "    if labels:\n",
    "        others_labels = np.unique(target_train).tolist()\n",
    "\n",
    "        correct_label = \",\".join(map(str, labels))\n",
    "        other_label = \",\".join([label for label in others_labels if label not in labels])\n",
    "        other_label = \"Other\" if len(other_label) > len(correct_label) * 1.5 else other_label\n",
    "\n",
    "        conversor = np.vectorize(lambda x: correct_label if x in labels else other_label)\n",
    "        data_train = conversor(data_train)\n",
    "        data_test = conversor(data_test)\n",
    "\n",
    "    labels = [str(label) for label in np.unique(data_train).tolist()]\n",
    "\n",
    "    print(f\"Model to classify [ {' | '.join(labels)} ]\")\n",
    "    if model is None:\n",
    "        # model = train_rnd_forest(input_train, data_train)\n",
    "        model = calc_time(train_rnd_forest)(input_train, data_train)\n",
    "    print_metrics(model, input_test, data_test)\n",
    "    return model, (data_train, data_test)\n",
    "\n",
    "\n",
    "print(\"FULL Data    \", np.unique(target_train))\n",
    "print()\n",
    "\n",
    "# full category model\n",
    "model, (data_train, data_test) = model_analisys(\n",
    "    input_train,\n",
    "    target_train,\n",
    "    input_test,\n",
    "    target_test,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Si tus datos tienen nombres de columnas (por ejemplo, si estás usando un DataFrame de pandas)\n",
    "# De lo contrario, puedes usar una lista con los nombres de las características\n",
    "features = input_train.columns\n",
    "\n",
    "# Crear un DataFrame con las características y sus importancias\n",
    "feature_importances = pl.DataFrame({\"Feature\": features, \"Importance\": importances})\n",
    "\n",
    "# Ordenar por importancia\n",
    "feature_importances = feature_importances.sort(by=\"Importance\", descending=True)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "display(feature_importances)\n",
    "\n",
    "# Graficar las importancias de las características\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.barh(feature_importances[\"Feature\"], feature_importances[\"Importance\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importances from Random Forest\")\n",
    "plt.gca().invert_yaxis()  # Para que las características más importantes estén en la parte superior\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "if True:\n",
    "    # Definir el modelo de Random Forest\n",
    "    rf_model = RandomForestClassifier(class_weight=\"balanced_subsample\", random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Definir el número de folds para la validación cruzada (por ejemplo, 5)\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Realizar la validación cruzada usando 'accuracy' como métrica\n",
    "    print(\"Iniciando validación cruzada\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        rf_model,\n",
    "        input_train,\n",
    "        target_train,\n",
    "        cv=kfold,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    # Mostrar los resultados\n",
    "    print(f\"Resultados de cada fold: {cv_scores}\")\n",
    "    print(f\"Media de accuracy: {np.mean(cv_scores):.2f}\")\n",
    "    print(f\"Desviación estándar de accuracy: {np.std(cv_scores):.2f}\")\n",
    "\n",
    "    print(\"---------------------------\")\n",
    "    print()\n",
    "\n",
    "    # Comparar la media con el rendimiento en el conjunto de prueba\n",
    "    print(\"Comparando con el conjunto de prueba\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    rf_model.fit(input_train, data_train)\n",
    "    test_score = rf_model.score(input_test.select(sorted(input_test.columns)), data_test)\n",
    "    print(f\"Accuracy en el conjunto de prueba: {test_score:.2f}\")\n",
    "\n",
    "    print(\"---------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
